{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional with MNIST (or Fashion MNIST)\n",
    "\n",
    "* `Conditional Generative Adversarial Nets`[arXiv:1411.1784](https://arxiv.org/abs/1411.1784)\n",
    "  * Mehdi Mirza and Simon Osindero\n",
    "  \n",
    "* This code is available to tensorflow version 2.0\n",
    "* Implemented by [`tf.keras.layers`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers) [`tf.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/losses)\n",
    "* Use `transposed_conv2d` and `conv2d` for Generator and Discriminator, respectively.\n",
    "  * I do not use `dense` layer for model architecture consistency. (So my architecture is different from original dcgan structure)\n",
    "* based on DCGAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:35.848701Z",
     "start_time": "2019-03-11T10:25:34.154960Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import PIL\n",
    "import imageio\n",
    "from IPython import display\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:37.342009Z",
     "start_time": "2019-03-11T10:25:37.336626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "model_name = 'cgan'\n",
    "train_dir = os.path.join('train', model_name, 'exp1')\n",
    "dataset_name = 'mnist'\n",
    "assert dataset_name in ['mnist', 'fashion_mnist']\n",
    "\n",
    "max_epochs = 50\n",
    "save_model_epochs = 10\n",
    "print_steps = 100\n",
    "save_images_epochs = 1\n",
    "batch_size = 256\n",
    "learning_rate_D = 1e-4\n",
    "learning_rate_G = 1e-3\n",
    "k = 1 # the number of step of learning D before learning G (Not used in this code)\n",
    "num_classes = 10 # number of classes for MNIST\n",
    "num_examples_to_generate = num_classes\n",
    "noise_dim = 100\n",
    "MNIST_SIZE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:40.144474Z",
     "start_time": "2019-03-11T10:25:39.638331Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "if dataset_name == 'mnist':\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.mnist.load_data()\n",
    "else:\n",
    "  (train_images, train_labels), _ = \\\n",
    "      tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(-1, MNIST_SIZE, MNIST_SIZE, 1).astype('float32')\n",
    "#train_images = train_images / 255. # Normalize the images to [0, 1]\n",
    "train_images = (train_images - 127.5) / 127.5 # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:41.939670Z",
     "start_time": "2019-03-11T10:25:41.935383Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_shape_and_type_for_labels(x, y):\n",
    "  y = tf.one_hot(y, depth=num_classes)\n",
    "  y = tf.reshape(y, shape=[1, 1, num_classes])\n",
    "  y = tf.cast(y, dtype = tf.float32)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:42.457405Z",
     "start_time": "2019-03-11T10:25:42.401371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((256, 28, 28, 1), (256, 1, 1, 10)), types: (tf.float32, tf.float32)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 21:13:55.537720: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-23 21:13:55.969693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22318 MB memory:  -> device: 0, name: GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#tf.random.set_seed(219)\n",
    "# for train\n",
    "N = len(train_images)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N)\n",
    "train_dataset = train_dataset.map(convert_shape_and_type_for_labels)\n",
    "train_dataset = train_dataset.batch(batch_size=batch_size, drop_remainder=True)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "## 5. Create the generator and discriminator models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTranspose(tf.keras.Model):\n",
    "  def __init__(self, filters, kernel_size, padding='same',\n",
    "               apply_batchnorm=True, activation='relu'):\n",
    "    super(ConvTranspose, self).__init__()\n",
    "    self.apply_batchnorm = apply_batchnorm\n",
    "    assert activation in ['relu', 'sigmoid', 'tanh']\n",
    "    self.activation = activation\n",
    "    self.up_conv = layers.Conv2DTranspose(filters=filters,\n",
    "                                          kernel_size=(kernel_size, kernel_size),\n",
    "                                          strides=2,\n",
    "                                          padding=padding,\n",
    "                                          kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                                          use_bias=not self.apply_batchnorm)\n",
    "    if self.apply_batchnorm:\n",
    "      self.batchnorm = layers.BatchNormalization()\n",
    "\n",
    "  def call(self, x, training=True):\n",
    "    # conv transpose\n",
    "    x = self.up_conv(x)\n",
    "    \n",
    "    # batchnorm\n",
    "    if self.apply_batchnorm:\n",
    "      x = self.batchnorm(x, training=training)\n",
    "      \n",
    "    # activation\n",
    "    if self.activation == 'relu':\n",
    "      x = tf.nn.relu(x)\n",
    "    elif self.activation == 'sigmoid':\n",
    "      x = tf.nn.sigmoid(x)\n",
    "    else:\n",
    "      x = tf.nn.tanh(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:43.854017Z",
     "start_time": "2019-03-11T10:25:43.844277Z"
    },
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "  \"\"\"Build a generator that maps latent space to real space given conditions.\n",
    "    G(z, c): (z, c) -> x\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "    self.conv1 = ConvTranspose(256, 3, padding='valid')\n",
    "    self.conv2 = ConvTranspose(128, 3, padding='valid')\n",
    "    self.conv3 = ConvTranspose(64, 4)\n",
    "    self.conv4 = ConvTranspose(1, 4, apply_batchnorm=False, activation='tanh')\n",
    "\n",
    "  def call(self, noise_inputs, conditions, training=True):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    # noise_inputs: [1, 1, 100]\n",
    "    # conditions: [1, 1, 10] (for MNIST)\n",
    "    # inputs = 1 x 1 x (100 + 10) dim\n",
    "    inputs = tf.concat([noise_inputs, conditions], axis=3)\n",
    "    conv1 = self.conv1(inputs, training=training)           # conv1: [3, 3, 256]\n",
    "    conv2 = self.conv2(conv1, training=training)            # conv2: [7, 7, 128]\n",
    "    conv3 = self.conv3(conv2, training=training)            # conv3: [14, 14, 64]\n",
    "    generated_images = self.conv4(conv3, training=training) # generated_images: [28, 28, 1]\n",
    "    \n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(tf.keras.Model):\n",
    "  def __init__(self, filters, kernel_size, strides, padding='same',\n",
    "               apply_batchnorm=True, activation='relu'):\n",
    "    super(Conv, self).__init__()\n",
    "    self.apply_batchnorm = apply_batchnorm\n",
    "    assert activation in ['relu', 'leaky_relu', 'none']\n",
    "    self.activation = activation\n",
    "        \n",
    "    self.conv = layers.Conv2D(filters=filters,\n",
    "                              kernel_size=(kernel_size, kernel_size),\n",
    "                              strides=strides,\n",
    "                              padding=padding,\n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.02),\n",
    "                              use_bias=not self.apply_batchnorm)\n",
    "    if self.apply_batchnorm:\n",
    "      self.batchnorm = layers.BatchNormalization()\n",
    "  \n",
    "  def call(self, x, training=True):\n",
    "    # convolution\n",
    "    x = self.conv(x)\n",
    "    \n",
    "    # batchnorm\n",
    "    if self.apply_batchnorm:\n",
    "      x = self.batchnorm(x, training=training)\n",
    "    \n",
    "    # activation\n",
    "    if self.activation == 'relu':\n",
    "      x = tf.nn.relu(x)\n",
    "    elif self.activation == 'leaky_relu':\n",
    "      x = tf.nn.leaky_relu(x)\n",
    "    else:\n",
    "      pass\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:44.414703Z",
     "start_time": "2019-03-11T10:25:44.407449Z"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):\n",
    "  \"\"\"Build a discriminator that discriminate tuple (x, c) whether real or fake.\n",
    "    D(x, c): (x, c) -> [0, 1]\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.conv1 = Conv(64, 4, 2, apply_batchnorm=False, activation='leaky_relu')\n",
    "    self.conv2 = Conv(128, 4, 2, activation='leaky_relu')\n",
    "    self.conv3 = Conv(256, 3, 2, padding='valid', activation='leaky_relu')\n",
    "    self.conv4 = Conv(1, 3, 1, padding='valid', apply_batchnorm=False, activation='none')\n",
    "\n",
    "  def call(self, image_inputs, conditions, training=True):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    # image_inputs: [28, 28, 1]\n",
    "    # conditions: 10 dim (for MNIST)\n",
    "    # inputs: [28, 28, (1 + 10)]\n",
    "    inputs = tf.concat([image_inputs,\n",
    "                        conditions * tf.ones([image_inputs.shape[0],\n",
    "                                              MNIST_SIZE, MNIST_SIZE,\n",
    "                                              num_classes])], axis=3)\n",
    "    conv1 = self.conv1(inputs)                            # conv1: [14, 14, 64]\n",
    "    conv2 = self.conv2(conv1)                             # conv2: [7, 7, 128]\n",
    "    conv3 = self.conv3(conv2)                             # conv3: [3, 3, 256]\n",
    "    conv4 = self.conv4(conv3)                             # conv4: [1, 1, 1]\n",
    "    discriminator_logits = tf.squeeze(conv4, axis=[1, 2]) # discriminator_logits: [1,]\n",
    "    \n",
    "    return discriminator_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:45.202315Z",
     "start_time": "2019-03-11T10:25:45.187007Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot generated image via generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 21:14:17.331318: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2021-12-23 21:14:18.037995: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f715328a640>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = tf.random.normal([1, 1, 1, noise_dim])\n",
    "condition = tf.one_hot(tf.random.uniform(shape=[], minval=0, maxval=10, dtype=tf.int32), depth=num_classes)\n",
    "condition = tf.reshape(condition, shape=[1, 1, 1, num_classes])\n",
    "generated_image = generator(noise, condition, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test discriminator network\n",
    "\n",
    "* **CAUTION**: the outputs of discriminator is **logits** (unnormalized probability) NOT probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.13022727]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "decision = discriminator(generated_image, condition)\n",
    "print(decision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define the loss functions and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:47.585554Z",
     "start_time": "2019-03-11T10:25:47.582509Z"
    }
   },
   "outputs": [],
   "source": [
    "# use logits for consistency with previous code I made\n",
    "# `tf.losses` and `tf.keras.losses` are the same API (alias)\n",
    "bce = tf.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:48.091644Z",
     "start_time": "2019-03-11T10:25:48.085602Z"
    }
   },
   "outputs": [],
   "source": [
    "def GANLoss(logits, is_real=True):\n",
    "  \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "\n",
    "  Args:\n",
    "    logits (`2-rank Tensor`): logits.\n",
    "    is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "\n",
    "  Returns:\n",
    "    loss (`0-rank Tensor`): the standard GAN loss value. (binary_cross_entropy)\n",
    "  \"\"\"\n",
    "  if is_real:\n",
    "    labels = tf.ones_like(logits)\n",
    "  else:\n",
    "    labels = tf.zeros_like(logits)\n",
    "\n",
    "  #return tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=logits)\n",
    "  return bce(labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:48.859139Z",
     "start_time": "2019-03-11T10:25:48.855085Z"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_logits, fake_logits):\n",
    "  # losses of real with label \"1\"\n",
    "  real_loss = GANLoss(logits=real_logits, is_real=True)\n",
    "  # losses of fake with label \"0\"\n",
    "  fake_loss = GANLoss(logits=fake_logits, is_real=False)\n",
    "  \n",
    "  return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:49.043515Z",
     "start_time": "2019-03-11T10:25:49.039490Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_logits):\n",
    "  # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "  return GANLoss(logits=fake_logits, is_real=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:49.219790Z",
     "start_time": "2019-03-11T10:25:49.215907Z"
    }
   },
   "outputs": [],
   "source": [
    "#discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate_D, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate_D)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate_G, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:25:50.620586Z",
     "start_time": "2019-03-11T10:25:50.615756Z"
    }
   },
   "outputs": [],
   "source": [
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement of the gan.\n",
    "# To visualize progress in the animated GIF\n",
    "const_random_vector_for_saving = tf.random.uniform([num_examples_to_generate, 1, 1, noise_dim],\n",
    "                                                   minval=-1.0, maxval=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training one step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  # generating noise from a uniform distribution\n",
    "  noise = tf.random.uniform([batch_size, 1, 1, noise_dim], minval=-1.0, maxval=1.0)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(noise, labels, training=True)\n",
    "\n",
    "    real_logits = discriminator(images, labels, training=True)\n",
    "    fake_logits = discriminator(generated_images, labels, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_logits)\n",
    "    disc_loss = discriminator_loss(real_logits, fake_logits)\n",
    "    \n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "  \n",
    "  return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train full steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-11T10:26:36.469395Z",
     "start_time": "2019-03-11T10:26:16.625832Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11.53 global_step: 2700 loss_D: 1.32 loss_G: 0.659 (24607.92 examples/sec; 0.010 sec/batch)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute '_from_compatible_tensor_list'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_550324/1451948780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_compatible_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m   return _from_tensor_list_helper(\n\u001b[0m\u001b[1;32m    243\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       element_spec, tensor_list)\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    217\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_spec_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 243\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m       element_spec, tensor_list)\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ste/lib/python3.8/site-packages/tensorflow/python/framework/tensor_spec.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# information.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Start Training.')\n",
    "num_batches_per_epoch = int(N / batch_size)\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "sample_condition = tf.eye(num_classes)\n",
    "sample_condition = tf.reshape(sample_condition, [-1, 1, 1, num_classes])\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "  \n",
    "  for step, (images, labels) in enumerate(train_dataset):\n",
    "    start_time = time.time()\n",
    "\n",
    "    gen_loss, disc_loss = train_step(images, labels)\n",
    "    global_step.assign_add(1)\n",
    "\n",
    "    if global_step.numpy() % print_steps == 0:\n",
    "      epochs = epoch + step / float(num_batches_per_epoch)\n",
    "      duration = time.time() - start_time\n",
    "      examples_per_sec = batch_size / float(duration)\n",
    "      display.clear_output(wait=True)\n",
    "      print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3g} loss_G: {:.3g} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                epochs, global_step.numpy(), disc_loss, gen_loss, examples_per_sec, duration))\n",
    "      noise = tf.random.uniform([num_examples_to_generate, 1, 1, noise_dim], minval=-1.0, maxval=1.0)\n",
    "      sample_images = generator(noise, sample_condition, training=False)\n",
    "      plt.figure(figsize=[18, 3])\n",
    "      for j in range(10):\n",
    "          plt.subplot(1, 10, j+1)\n",
    "          plt.imshow(sample_images[j, :, :, 0], cmap='binary')\n",
    "          plt.xticks([])\n",
    "          plt.yticks([])\n",
    "      plt.show()\n",
    "\n",
    "print('Training Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "413px",
    "left": "733px",
    "right": "2px",
    "top": "77px",
    "width": "493px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
